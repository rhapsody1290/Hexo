---
title: 构建高可用的系统

date: 2017-06-26 21:46:00

categories:
- 分布式架构

tags:
- 高可用

---

![](http://i.imgur.com/NO7vRM7.jpg)

## 负载均衡

负载均衡分为硬件负载均衡和软件负载均衡

### 防止单点

负载均衡为避免自己成为单点，通常由两台机器构成，但只有一台处于服务状态，另一台则处于standby状态。一旦处于服务的那台机器出现问题，standby这台会自动接管

![](http://i.imgur.com/Q2IQmQe.jpg)

### 负载均衡策略

用户发送请求到负载均衡机器，负载均衡机器再将请求转发给实际的业务处理机器，通常负载均衡机器知道实际业务处理机器的IP地址，选择的方式如下：

1、随机选择
2、**Hash选择**。对应用层的请求信息做hash，这样可以保证每次请求的都是同一台机器，命中缓存。如查询图片
3、**Round-Robin选择**。按照处理机器的IP地址列表顺序选择，为了保持顺序选择时需要同步操作，但由于操作时间段性能损失很小，这种方式硬件负载和软件负载都支持，**实际中使用最多**
4、按权重选择。根据每个地址的权重进行选择
5、按负载选择。根据实际业务处理机器的负载来选择，选择负载相对较低的机器来处理
6、按连接选择，让连接数相对较少的机器来处理业务，如果重启一台机器，可能会瞬间接收到大量请求，造成机器宕掉，这种方式实际应用很少

### 跳过问题机器

为保证访问时跳过出问题的机器，通常采用的方法是负载均衡机器定时和实际的业务处理机器进行**心跳**（ping、端口检测、url侦测），发现心跳失败的机器并将它**从可用地址列表中拿掉，在心跳成功后再重新加入可用列表中**

### 响应返回方式

业务处理机器处理完毕后，要将响应返回给用户，通常有两种返回方式：

1、响应通过负载均衡机器返回

**基于NAT实现**，当请求从客户端发送至负载均衡机器时，负载均衡机器选择一台实际的业务处理机器，然后将**请求报文的目标地址和端口改为实际业务处理机器的IP地址和端口**，并将报文发送出去。当响应回到负载均衡机器上时，将报文的源地址和端口修改为负载均衡机器的VIP地址和端口

![](http://i.imgur.com/TJ5MAmZ.jpg)

2、响应直接返回至请求发起方

响应直接返回至请求发起方可将请求包和响应包分开处理，以分散负载均衡机器的压力，使负载均衡机器可以支撑更大的请求量。要达到响应直接返回的效果，须要采用IP Tunneling或DR（Direct Routing，硬件负载设备中又简写为DSR：Direct Service Routing），这两种方式对负载均衡机器和实际业务处理机器的系统环境都有要求

①IP Tunneling

当采用IP Tunneling方式时，请求从客户端发送至负载均衡机器，负载均衡机器首先选择一台实际的业务处理机器，然后将请求的IP报文基于IP封装技术封装成另外一个IP报文，在做完以上处理后将报文发送出去，实际的业务处理机器收到报文后，先将报文解开获得目标地址为VIP的报文，处理完毕请求后，处理机器发现此VIP地址配置在本地的IP隧道设备上，则根据路由表将响应报文直接返回至请求方。IP Tunneling方式要求负载均衡机器和实际的业务处理机器的os都支持IP Tunneling，并将VIP地址同时配置在实际业务处理机器的IP隧道设备上

![](http://i.imgur.com/CLTQdf4.jpg)

②Direct Routing

当采用Direct Routing方式时，请求从客户端发送至负载均衡机器，负载均衡机器首先选择一台实际的业务处理机器，然后将请求数据帧的MAC地址修改为此业务处理机器的MAC地址，并发送出去，实际的业务处理机器收到请求后，获取IP报文，当发现IP报文中的目标地址VIP配置在本地的网络设备上时，根据路由表将响应报文直接返回给用户。Direct Routing方式要求负载均衡机器和实际的业务处理机器在同一个物理网段中，并且不响应ARP

![](http://i.imgur.com/97MaQPD.jpg)

根据上面的描述可以看出，IP Tunneling方式对系统环境的要求并不高，目前大部分的OS都支持IP Tunneling，Direct Routing方式对系统环境的要求则比较高，因此IP Tunneling方式更适合实现将响应直接返回给请求发起方，从而大幅度提升负载均衡机器所能支撑的请求量。

### 软件负载

1、LVS+KeepAlived

软件负载方案中最常用的为LVS（Linux Virtual Server），多数情况下采取**LVS+Keepalived来避免负载均衡机器的单点，实现负载均衡机器的自动接管**。

Keepalived基于**VRRP**（Virtual Router Redundancy Protocol）协议实现，在VRRP协议中，由一个Master的VRRP路由器和多个Backup的VRRP路由器构成**VRRP虚拟路由器**，但Master并不是永远不变的，Master的VRRP路由器会每隔一段时间发送广播包。当Backup VRRP路由器在连续三个周期内都收不到广播包时，即认为Master VRRP路由器出现问题，或收到优先级为0的广播包后，所有Backup VRRP路由器都发送VRRP广播信息，声称自己是Master，**并将虚拟IP增加到当前机器上，从而保持对外提供的IP地址及MAC地址不变。**Backup VRRP路由器收到VRRP广播信息后，首先比较优先级，如优先级比收到的VRRP广播信息中的优先级低，则重新将状态置为BACKUP。如优先级相等，则比较IP地址，IP值小的则重新将状态恢复为BACKUP，整个切换过程对于请求端而言是透明的。但由于VRRP方式依靠广播信息来确认是否健康，如网络上出现异常，有可能会出现多个Master的现象，这个时候会出现一些问题，因此当使用VRRP方式时要特别监测是否出现此类现象，一旦出现就要迅速人工介入处理。

2、硬件负载设备

除了采用Keepalived方式实现自动接管外，也可采用类似硬件负载设备的方式来实现，即**采用心跳线+高可用软件来实现**。在linux目前使用范畴最广的高可用软件为heartbeat，默认情况下heartbeat通过UDP方式来监测。

除LVS外，软件负载方案中还有像HAProxy这样的佼佼者，在考察采用哪种软件负载方案时，则要从应用场景、系统环境等多方面考虑。

### 故障传播

在系统从单机演变为集群后，可用性确实会得到一定提升，但随着系统功能的不断丰富，会出现**多个系统访问同一系统提供的功能**的情况，在这种情况下有可能会出现其中某个系统的访问导致其他系统故障。

对于以上这种故障传播的现象，通常会根据应用性质的不同做隔离的方案，通常采取配置多个不同的VIP的方法，**各个系统通过域名访问，通过dns等方法使域名根据不同的系统解析为不同的VIP**，从而实现根据应用性质不同来隔离，避免故障传播。

## 热备

热备通常对程序的要求不高，**热备的情况下真正对外服务的机器只有一台，其他机器处于standby状态。**standby机器通过心跳机制检查对外服务机器的健康状况，当出现问题时，其中一台standby机器即进行接管，机器间的状态同步至其他standby机器或写入一个集中存储设备，例如上述章节中LVS+Keepalived实现自动接管的方式

对于大型应用而言，除了单机故障外，**还须考虑整个机房出现不可用的情况。如所有的应用都部署在单个机房，也可以认为是单点现象，**一旦发生机房断电或机房出现不可抗力的灾难性事故时，整个系统的可用性就完全无法保障了，对于此类现象，通常采用多个机房的方法来避免，一方面可以做到其中一个机房出现问题时对整个系统不会产生太大的影响，另一方面也可以分流，提升性能。

## 使用多机房——保持一致性

❶、主从同步

数据库数据的同步通常采用 **单master、多slave** 或 **多master方案**。单master方案只有一个写入点，其主要解决的是master同步到slave的问题，通常采取的是**数据库自带的同步方案**，例如oracle standby方案或mysql replication方案。


❷、分布式同步

多master方案有多个写入点，相对单master方案就复杂多了，通常采取的两阶段提交、三阶段提交或基于Paxos的方式来保持多master数据的一致性。

1、两阶段提交（2PC）保持一致性

在采用两阶段提交保证多master数据的一致性时，步骤为：

	1）开启事务；
	2）通知每个master执行某操作；
	3）所有master在接到请求后，锁定执行此操作需要的资源，例如假设是个扣款动作，那么先冻结相应的款项，冻结完毕后返回；
	4）在收到所有master的反馈后，如均为可执行此操作，则继续之后的步骤，如有一个master反馈不能执行或一段时间内无反馈，则通知所有master回滚操作；
	5）通知所有master完成操作。
	两阶段提交方式相对而言比较易于实现，但问题在于所有的master都要冻结资源，而且一旦有一个master出现问题就要全部回滚。

2、三阶段提交（3PC）保持一致性

为了避免在通知所有master提交时，其中一个master crash不一致时，就出现了三阶段提交的方式。三阶段提交在两阶段提交的基础上增加了preCommit的过程，**当所有master收到preCommit后，并不执行动作，直到收到commit或超过一定时间后才完成操作。**

在实现两阶段或三阶段提交时，为了避免通知所有master时出现问题，通常会借助消息中间件或让任意的一个master能够接管成为通知者。

3、基于Paxos保持一致性

Paxos最大的改变在于不要求所有master都反馈成功，**只须有大多数反馈成功就执行了**，更多具体的细节请参考相关文献。

总结：

文件的同步和内存数据的同步采取的方案和数据库数据同步的方案基本相同。
总的来说，由于采用多机房后带来的网络延时问题，技术上会出现不少的挑战，不过对于要求高可用的应用，采用多机房还是很有必要的。

## 分布式文件系统

分布式文件系统采用的方法由众多普通PC Server机器构成巨大的存储池，**每台机器只存储一部分数据**，其本身通常可非常好地支持水平伸缩。例如一台机器能存储500GB数据，那么当要存储2000GB数据时，只要增加到四台机器即可

![](http://i.imgur.com/Gj27UUY.jpg)

当Node A要上传文件时，Node A上的GFS Client会将文件按固定大小划分，并向主服务器提交**文件名和块索引信息，从而得到要存储的目标机器及位置**，主服务器根据目前各存储机器的存活状态、硬件使用率等来决定块需要存储到的目标机器，之后Node A将数据存储到目标机器的相应位置上。主服务器负责记录文件和块的命名空间、文件到块的映射及每个块副本的位置。

为了保证安全可靠，同时**将数据复制到多个存储机器上**，复制的份数可在主服务器上进行设置，当Node B要读取此文件时，则只要从主服务器上获取此文件划分的存储位置列表，然后随机挑选机器进行读取，最后**根据块的索引进行合并即可**。

## 应用水平伸缩

在系统建设初期，会采用将各种业务都放在同一个系统的方式，这会导致这个系统日渐庞大，所需的资源（CPU、内存、数据库连接）越来越多，在进行水平伸缩时要考虑系统里各种业务会造成的资源增加的现象，这种状况会导致水平伸缩很难进行。例如增加机器后就造成了多个数据库连接的增加，对于这样的状况，通常采取拆分应用的方式来解决。

**拆分应用通常按照业务领域来划分**，即将原在同一系统中处理的功能拆分到各个不同的业务系统中，例如eBay将其业务系统拆分为商品、用户、评价、交易等

![](http://i.imgur.com/XY3YCaY.jpg)